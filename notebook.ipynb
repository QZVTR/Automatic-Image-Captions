{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1877bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm \n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9b69c",
   "metadata": {},
   "source": [
    "# Download NLTK Tokeniser\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     C:\\Users\\Edward\\AppData\\Roaming\\nltk_data...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f397e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Edward/nltk_data', 'c:\\\\Users\\\\Edward\\\\Desktop\\\\Projects\\\\ML\\\\Automatic Image Captioning\\\\venv\\\\nltk_data', 'c:\\\\Users\\\\Edward\\\\Desktop\\\\Projects\\\\ML\\\\Automatic Image Captioning\\\\venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\Edward\\\\Desktop\\\\Projects\\\\ML\\\\Automatic Image Captioning\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Edward\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Edward\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f439a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Edward\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8880cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['.DS_Store', 'czech.pickle', 'danish.pickle', 'dutch.pickle', 'english.pickle', 'estonian.pickle', 'finnish.pickle', 'french.pickle', 'german.pickle', 'greek.pickle', 'italian.pickle', 'malayalam.pickle', 'norwegian.pickle', 'polish.pickle', 'portuguese.pickle', 'PY3', 'README', 'russian.pickle', 'slovene.pickle', 'spanish.pickle', 'swedish.pickle', 'turkish.pickle']\n"
     ]
    }
   ],
   "source": [
    "punkt_path = 'C:/Users/Edward/AppData/Roaming/nltk_data/tokenizers/punkt'\n",
    "print(os.path.exists(punkt_path))\n",
    "print(os.listdir(punkt_path) if os.path.exists(punkt_path) else \"Punkt not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d3b43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('captions_csv.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2fd848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captionsDf = df['caption']\n",
    "imageNamesDf = df['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1208327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A child in a pink dress is climbing up a set o...\n",
       "1                A girl going into a wooden building .\n",
       "2     A little girl climbing into a wooden playhouse .\n",
       "3    A little girl climbing the stairs to her playh...\n",
       "4    A little girl in a pink dress going into a woo...\n",
       "Name: caption, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captionsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82603d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000268201_693b08cb0e.jpg\n",
       "1    1000268201_693b08cb0e.jpg\n",
       "2    1000268201_693b08cb0e.jpg\n",
       "3    1000268201_693b08cb0e.jpg\n",
       "4    1000268201_693b08cb0e.jpg\n",
       "Name: image, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageNamesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e04be845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imageNamesDf) == len(captionsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "519d970f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40455 entries, 0 to 40454\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   image    40455 non-null  object\n",
      " 1   caption  40455 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 632.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a64c7b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40455, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76b8d7",
   "metadata": {},
   "source": [
    "# Vocabulary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3622644e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455</td>\n",
       "      <td>40455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8091</td>\n",
       "      <td>40201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>Two dogs playing in the snow .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image                         caption\n",
       "count                      40455                           40455\n",
       "unique                      8091                           40201\n",
       "top     997722733_0cb5439472.jpg  Two dogs playing in the snow .\n",
       "freq                           5                               7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6576be1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image      0\n",
       "caption    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49395e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freqThreshold=5):\n",
    "        #print(\"Vocabulary __init__ called. Ensuring itos has integer keys.\") # <--- ADD THIS LINE\n",
    "\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freqThreshold = freqThreshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def buildVocabulary(self, sentenceList):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # Start new word indices from 4\n",
    "        for sentence in sentenceList:\n",
    "            for word in nltk.tokenize.word_tokenize(sentence.lower()):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] >= self.freqThreshold and word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    \n",
    "                    idx += 1\n",
    "\n",
    "    def numericalise(self, text):\n",
    "        tokenisedText = nltk.tokenize.word_tokenize(text.lower())\n",
    "        # The .get() method here handles unknown words by defaulting to <UNK>'s index\n",
    "        return [self.stoi.get(token, self.stoi['<UNK>']) for token in tokenisedText]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311db8b2",
   "metadata": {},
   "source": [
    "# Custom Dataset Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6803d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCustom(Dataset):\n",
    "    def __init__(self, rootDir, captions, imageNames, transform=None, freqThreshold=5):\n",
    "        self.rootDir = rootDir\n",
    "        self.transform = transform\n",
    "        self.vocab = Vocabulary(freqThreshold)\n",
    "        \n",
    "        # Load captions\n",
    "        self.imgs = []\n",
    "        self.captions = []\n",
    "\n",
    "        for i in range(len(captions)):\n",
    "            self.imgs.append(imageNames[i])\n",
    "            self.captions.append(captions[i])\n",
    "        \n",
    "        #print(f'first 5 images: {self.imgs[:5]}')\n",
    "        # Build vocabulary\n",
    "        self.vocab.buildVocabulary(self.captions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgPath = os.path.join(self.rootDir, self.imgs[idx])\n",
    "        img = Image.open(imgPath).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        caption = self.captions[idx]  # Get one caption (randomly select or iterate)\n",
    "        numericalisedCaption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalisedCaption += self.vocab.numericalise(caption)  # Use first caption\n",
    "        numericalisedCaption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        return img, torch.tensor(numericalisedCaption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb61f53",
   "metadata": {},
   "source": [
    "# Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9c4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('C:/Users/Edward/AppData/Roaming/nltk_data/tokenizers/punkt')  # Update to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0be18e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary __init__ called. Ensuring itos has integer keys.\n",
      "Vocabulary __init__ called. Ensuring itos has integer keys.\n"
     ]
    }
   ],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets for train, validation, and test\n",
    "trainDataset = Flickr8kCustom(\n",
    "    rootDir='Images/',\n",
    "    captions=captionsDf,\n",
    "    imageNames=imageNamesDf,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "valDataset = Flickr8kCustom(\n",
    "    rootDir='Images/',\n",
    "    captions=captionsDf,\n",
    "    imageNames=imageNamesDf,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951b048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ff6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(trainDataset.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1305bf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 40455\n",
      "Validation dataset size: 40455\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset size: {len(trainDataset)}\")\n",
    "print(f\"Validation dataset size: {len(valDataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ca5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224])\n",
      "Caption indices: tensor([  1,   4,  28,   8,   4, 190, 148,  17,  32,  67,   4, 347,  11, 703,\n",
      "          8,  24,   3, 492,   5,   2])\n",
      "3005\n",
      "Caption: <SOS> a child in a pink dress is climbing up a set of stairs in an <UNK> way . <EOS>\n"
     ]
    }
   ],
   "source": [
    "img, caption = trainDataset[0]\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Caption indices: {caption}\")\n",
    "#print(len(trainDataset.vocab.itos))\n",
    "print(f\"Caption: {' '.join([trainDataset.vocab.itos[idx.item()] for idx in caption])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ab2ac",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d058c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedSize):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        #resnet = models.resnet18(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedSize)\n",
    "        self.bn = nn.BatchNorm1d(embedSize, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d71e7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedSize, hiddenSize, vocabSize, numLayers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocabSize, embedSize)\n",
    "        self.lstm = nn.LSTM(embedSize, hiddenSize, numLayers, batch_first=True)\n",
    "        self.linear = nn.Linear(hiddenSize, vocabSize)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d496c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embedSize, hiddenSize, vocabSize, numLayers=1):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embedSize)\n",
    "        self.decoder = DecoderRNN(embedSize, hiddenSize, vocabSize, numLayers)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def captionImages(self, image, vocabulary, maxLength=50):\n",
    "        resultCaption = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(1)\n",
    "            states = None\n",
    "            for _ in range(maxLength):\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)\n",
    "                resultCaption.append(predicted.item())\n",
    "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "                if predicted.item() == vocabulary.stoi['<EOS>']:\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in resultCaption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "343cb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collateFn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b154c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, loss_fn, optimizer, vocab, device):\n",
    "    numEpochs = 10\n",
    "    batchSize = 32\n",
    "\n",
    "\n",
    "    trainLoader = DataLoader(\n",
    "        trainDataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=True,\n",
    "        collate_fn=collateFn\n",
    "    )\n",
    "\n",
    "    valLoader = DataLoader(\n",
    "        valDataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=False,\n",
    "        collate_fn=collateFn\n",
    "    )\n",
    "\n",
    "    #Training Loop\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        model.train()\n",
    "        totalTrainLoss = 0\n",
    "\n",
    "        for images, captions, lengths in tqdm.tqdm(trainLoader, desc=f'Epoch {epoch+1}'):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            outputs = model(images, captions)\n",
    "            loss = loss_fn(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            totalTrainLoss += loss.item()\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        totalValLoss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions, lengths in valLoader:\n",
    "                images, captions = images.to(device), captions.to(device)\n",
    "                outputs = model(images, captions)\n",
    "                loss = loss_fn(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "                totalValLoss+= loss.item()\n",
    "\n",
    "        \n",
    "        print(\n",
    "            f'Epoch [{epoch+1} / {numEpochs}], '\n",
    "            f'TrainLoss: {totalTrainLoss / len(trainLoader):.4f}, '\n",
    "            f'Val Loss: {totalValLoss/len(valLoader):.4f}'    \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b199821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2a826d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edward\\Desktop\\Projects\\ML\\Automatic Image Captioning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Edward\\Desktop\\Projects\\ML\\Automatic Image Captioning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialise Model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CNNtoRNN(256, 512, len(vocab), 1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<PAD>'])\n",
    "learningRate = 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c72715c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1265/1265 [03:15<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10], TrainLoss: 3.4279, Val Loss: 2.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1265/1265 [03:13<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2 / 10], TrainLoss: 2.7247, Val Loss: 2.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1265/1265 [03:16<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3 / 10], TrainLoss: 2.4786, Val Loss: 2.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1265/1265 [03:17<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4 / 10], TrainLoss: 2.3190, Val Loss: 2.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1265/1265 [03:12<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5 / 10], TrainLoss: 2.1961, Val Loss: 2.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1265/1265 [03:11<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6 / 10], TrainLoss: 2.0928, Val Loss: 1.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1265/1265 [03:12<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7 / 10], TrainLoss: 2.0019, Val Loss: 1.8807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1265/1265 [03:12<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8 / 10], TrainLoss: 1.9193, Val Loss: 1.7983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1265/1265 [03:12<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9 / 10], TrainLoss: 1.8420, Val Loss: 1.7271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1265/1265 [03:12<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10 / 10], TrainLoss: 1.7705, Val Loss: 1.6418\n"
     ]
    }
   ],
   "source": [
    "trainModel(model, loss_fn, optimizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9d88ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'auto_caption_model_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82af7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary __init__ called. Ensuring itos has integer keys.\n"
     ]
    }
   ],
   "source": [
    "testDataset = Flickr8kCustom(\n",
    "    rootDir='Images/',\n",
    "    captions=captionsDf,\n",
    "    imageNames=imageNamesDf,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testLoader = DataLoader(testDataset, batch_size=32, shuffle=False, collate_fn=collateFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f84f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataLoader, vocab, device):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, length in dataLoader:\n",
    "            images = images.to(device)\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].unsqueeze(0).to(device)\n",
    "                caption = model.captionImages(image, vocab)\n",
    "                hypotheses.append(caption)\n",
    "                ref = [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2, 3]]\n",
    "                references.append([ref])\n",
    "    \n",
    "    blueScore = corpus_bleu(references, hypotheses)\n",
    "    print(f'BLEU Score: {blueScore:.4f}')\n",
    "    return blueScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a72f4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0786\n"
     ]
    }
   ],
   "source": [
    "res = evaluateModel(model, testLoader, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a4e5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCaption(imagePath, model, vocab, transform, device, maxLength=50):\n",
    "    if not isinstance(imagePath, str):\n",
    "        raise TypeError(f\"imagePath must be a string, got {type(imagePath)}: {imagePath}\")\n",
    "    \n",
    "    model.eval()\n",
    "    image = Image.open(imagePath).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    caption = model.captionImages(image, vocab, maxLength)\n",
    "    captionWords = [word for word in caption if word not in [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]]\n",
    "    caption = ' '.join(captionWords).strip()\n",
    "\n",
    "    for p in string.punctuation:\n",
    "        caption.replace(f\" {p}\", p)\n",
    "\n",
    "    if caption:\n",
    "        caption = caption[0].upper() + caption[1:]\n",
    "\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "801784d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: Two dogs are playing in a field .\n"
     ]
    }
   ],
   "source": [
    "# Images/667626_18933d713e.jpg # girl in water\n",
    "# Images/23445819_3a458716c1.jpg two dogs\n",
    "# Images/132489044_3be606baf7.jpg man sleeping\n",
    "# Images/61209225_8512e1dad5.jpg re-enactment\n",
    "# Images/95728660_d47de66544.jpg man cycling in mountains\n",
    "caption = generateCaption('Images/23445819_3a458716c1.jpg', model, vocab, transform, device)\n",
    "print(f'Generated Caption: {caption}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9750266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapText(caption, font, draw, max_width):\n",
    "    lines = []\n",
    "    words = caption.split(' ')\n",
    "    currentLine = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Test adding word to current line\n",
    "        textLine = ' '.join(currentLine + [word])\n",
    "        textBBox = draw.textbbox((0, 0), textLine, font=font)\n",
    "        textWidth = textBBox[2] - textBBox[0]\n",
    "        \n",
    "        if textWidth <= max_width:\n",
    "            currentLine.append(word)\n",
    "        else:\n",
    "            if currentLine:\n",
    "                lines.append(' '.join(currentLine))\n",
    "            currentLine = [word]\n",
    "    \n",
    "    if currentLine:\n",
    "        lines.append(' '.join(currentLine))\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02764589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCaptionToImage(inputImagePath, outputImagePath, model, vocab, transform, device, max_length=50):\n",
    "\n",
    "    # Ensure inputImagePath is a string\n",
    "    if not isinstance(inputImagePath, str):\n",
    "        raise TypeError(f\"inputImagePath must be a string, got {type(inputImagePath)}: {inputImagePath}\")\n",
    "    \n",
    "    # Debug: Print input path\n",
    "    print(f\"Input image path: {inputImagePath}\")\n",
    "    if not os.path.exists(inputImagePath):\n",
    "        raise FileNotFoundError(f\"Image not found: {inputImagePath}\")\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = generateCaption(inputImagePath, model, vocab, transform, device, max_length)\n",
    "    \n",
    "    # Open the input image\n",
    "    image = Image.open(inputImagePath).convert('RGB')\n",
    "    \n",
    "    # Load font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20) \n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    \n",
    "    maxTextWidth = image.width - 20  # 10px padding on each side\n",
    "    captionLines = wrapText(caption, font, draw, maxTextWidth)\n",
    "    \n",
    "    \n",
    "    textBBox = draw.textbbox((0, 0), \"Sample\", font=font)\n",
    "    lineHeight = textBBox[3] - textBBox[1]\n",
    "    captionHeight = len(captionLines) * lineHeight + (len(captionLines) - 1) * 5 + 20  # 5px spacing, 10px padding top/bottom\n",
    "    \n",
    "    # Create new image with space for caption\n",
    "    newImage = Image.new('RGB', (image.width, image.height + captionHeight), color='white')\n",
    "    newImage.paste(image, (0, captionHeight))\n",
    "    \n",
    "    \n",
    "    draw = ImageDraw.Draw(newImage)\n",
    "    \n",
    "    \n",
    "    yPosition = 10  # Top padding\n",
    "    for line in captionLines:\n",
    "        textBBox = draw.textbbox((0, 0), line, font=font)\n",
    "        textWidth = textBBox[2] - textBBox[0]\n",
    "        textX = (image.width - textWidth) // 2\n",
    "        \n",
    "        outlineColour = 'white'\n",
    "        fillColour = 'black'\n",
    "        offset = 1\n",
    "        for dx, dy in [(-offset, -offset), (-offset, offset), (offset, -offset), (offset, offset)]:\n",
    "            draw.text((textX + dx, yPosition + dy), line, font=font, fill=outlineColour)\n",
    "        draw.text((textX, yPosition), line, font=font, fill=fillColour)\n",
    "        yPosition += lineHeight + 5  # Line spacing\n",
    "    \n",
    "   \n",
    "    newImage.save(outputImagePath)\n",
    "    print(f'Saved captioned image to: {outputImagePath}')\n",
    "    \n",
    "    return newImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "25bd49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImagePath = 'Images/667626_18933d713e.jpg'\n",
    "outputImagePath = 'Output_Images/667626_18933d713e_captioned_output.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19ac4290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image path: Images/667626_18933d713e.jpg\n",
      "Saved captioned image to: Output_Images/667626_18933d713e_captioned_output.jpg\n",
      "Caption: A young girl is in the water .\n"
     ]
    }
   ],
   "source": [
    "captionedImage = addCaptionToImage(inputImagePath, outputImagePath, model, vocab, transform, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
